{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png)  Capstone Project:  \"Does Wealth = Health?\"\n",
    "*Predicting health based on indicators of financial wellbeing*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this notebook covers modeling. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic Python libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# This will allow us to avoid a FutureWarning when plotting.\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the dataset of (40) selected features ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6341, 41)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PUF_ID</th>\n",
       "      <th>HEALTH</th>\n",
       "      <th>SWB_1</th>\n",
       "      <th>SWB_2</th>\n",
       "      <th>SWB_3</th>\n",
       "      <th>FWBscore</th>\n",
       "      <th>FWB1_1</th>\n",
       "      <th>FWB1_2</th>\n",
       "      <th>FWB1_4</th>\n",
       "      <th>FWB2_2</th>\n",
       "      <th>...</th>\n",
       "      <th>HHEDUC</th>\n",
       "      <th>PPINCIMP</th>\n",
       "      <th>PPREG4</th>\n",
       "      <th>PPREG9</th>\n",
       "      <th>fpl</th>\n",
       "      <th>agecat</th>\n",
       "      <th>generation</th>\n",
       "      <th>PPGENDER</th>\n",
       "      <th>PPMARIT</th>\n",
       "      <th>PPETHM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10350</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7740</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13699</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7375</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10910</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PUF_ID  HEALTH  SWB_1  SWB_2  SWB_3  FWBscore  FWB1_1  FWB1_2  FWB1_4  \\\n",
       "0   10350       1      5      5      6        55       3       3       3   \n",
       "1    7740       1      6      6      6        51       2       2       3   \n",
       "2   13699       1      4      3      4        49       3       3       3   \n",
       "3    7375       1      4      4      4        49       3       3       3   \n",
       "4   10910       0      5      7      5        67       5       1       1   \n",
       "\n",
       "   FWB2_2  ...  HHEDUC  PPINCIMP  PPREG4  PPREG9  fpl  agecat  generation  \\\n",
       "0       3  ...       4         7       4       8    3       8           1   \n",
       "1       2  ...       2         6       2       3    3       3           3   \n",
       "2       3  ...       3         6       4       9    3       3           3   \n",
       "3       3  ...       2         7       2       4    3       2           4   \n",
       "4       5  ...       4         7       2       3    3       2           4   \n",
       "\n",
       "   PPGENDER  PPMARIT  PPETHM  \n",
       "0         1        3       1  \n",
       "1         1        3       1  \n",
       "2         1        3       2  \n",
       "3         1        1       3  \n",
       "4         1        1       1  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data into dataframe from .csv file exported in the previous notebook. . .\n",
    "\n",
    "#Adjust the path to where you have saved the data\n",
    "df_feat = pd.read_csv(\"../data/NFWBS_40_FEATURES_export.csv\")\n",
    "print(df_feat.shape)\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HEALTH</th>\n",
       "      <th>SWB_1</th>\n",
       "      <th>SWB_2</th>\n",
       "      <th>SWB_3</th>\n",
       "      <th>FWBscore</th>\n",
       "      <th>FWB1_1</th>\n",
       "      <th>FWB1_2</th>\n",
       "      <th>FWB1_4</th>\n",
       "      <th>FWB2_2</th>\n",
       "      <th>FSscore</th>\n",
       "      <th>...</th>\n",
       "      <th>HHEDUC</th>\n",
       "      <th>PPINCIMP</th>\n",
       "      <th>PPREG4</th>\n",
       "      <th>PPREG9</th>\n",
       "      <th>fpl</th>\n",
       "      <th>agecat</th>\n",
       "      <th>generation</th>\n",
       "      <th>PPGENDER</th>\n",
       "      <th>PPMARIT</th>\n",
       "      <th>PPETHM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUF_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10350</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7740</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13699</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7375</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10910</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        HEALTH  SWB_1  SWB_2  SWB_3  FWBscore  FWB1_1  FWB1_2  FWB1_4  FWB2_2  \\\n",
       "PUF_ID                                                                          \n",
       "10350        1      5      5      6        55       3       3       3       3   \n",
       "7740         1      6      6      6        51       2       2       3       2   \n",
       "13699        1      4      3      4        49       3       3       3       3   \n",
       "7375         1      4      4      4        49       3       3       3       3   \n",
       "10910        0      5      7      5        67       5       1       1       5   \n",
       "\n",
       "        FSscore  ...  HHEDUC  PPINCIMP  PPREG4  PPREG9  fpl  agecat  \\\n",
       "PUF_ID           ...                                                  \n",
       "10350        44  ...       4         7       4       8    3       8   \n",
       "7740         43  ...       2         6       2       3    3       3   \n",
       "13699        42  ...       3         6       4       9    3       3   \n",
       "7375         42  ...       2         7       2       4    3       2   \n",
       "10910        57  ...       4         7       2       3    3       2   \n",
       "\n",
       "        generation  PPGENDER  PPMARIT  PPETHM  \n",
       "PUF_ID                                         \n",
       "10350            1         1        3       1  \n",
       "7740             3         1        3       1  \n",
       "13699            3         1        3       2  \n",
       "7375             4         1        1       3  \n",
       "10910            4         1        1       1  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set PUF_ID as index\n",
    "df_feat.set_index('PUF_ID', inplace=True)\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start modeling (with no preprocessing or rebalancing)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python classifcation model libraries\n",
    "\n",
    "# for Logistic Regression (classification)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# for Knn (classification)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# for Train/Test K-Fold Cross-Validation scoring... \n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# compute classification accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "# Confusion matrix measures results, so test data is used\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization is not necessary when variables are already on the same scale\n",
    "\n",
    "https://towardsdatascience.com/when-to-standardize-your-data-in-4-minutes-f9282190707e\n",
    "Standardization is not required for Logistic Regression and Tree based algorithms \n",
    "such as Decision Tree, Random forest and gradient boosting, because they are not\n",
    "sensitive to the magnitude of variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, per sklearn (Logistic Regression) documentation: \"Note that regularization is applied by default.\"\n",
    "penalty : str, ‚Äòl1‚Äô, ‚Äòl2‚Äô, ‚Äòelasticnet‚Äô or ‚Äònone‚Äô, optional (default=‚Äôl2‚Äô)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression overfitting\n",
    "Models are overfit when the test score is worse than the train score.\n",
    "\n",
    "\n",
    "### In order to address overfitting with Logistic Regression, we can try to:\n",
    "### 1. adjust regularization like Lasso and Ridge (as seen in the models above)\n",
    "### 2. reduce the number of variables / level of complexity\n",
    "### 3. increase the amount of data (if possible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "TRAIN TEST SPLIT (80/20)\n",
      "------------------------\n",
      "X_train data:  (5072, 39)\n",
      "X_test data:  (1269, 39)\n",
      "------------------------\n",
      "TEST SCORES\n",
      "------------------------\n",
      "logreg Test accuracy score:  0.6832151300236406\n",
      "logreg Test f1 score:  0.6649999999999999\n",
      "logreg Test recall score:  0.6456310679611651\n",
      "------------------------\n",
      "CONFUSION MATRIX\n",
      "------------------------\n",
      "TOTAL negative (0: \"optimal health\") =  687\n",
      "true negative =  468\n",
      "false negative =  219\n",
      "TOTAL positive (1: \"sub-optimal health\") =  582\n",
      "false positive =  183\n",
      "true positive =  399\n",
      "[[468 183]\n",
      " [219 399]]\n"
     ]
    }
   ],
   "source": [
    "# Separate input features and target\n",
    "y = df_feat.HEALTH\n",
    "X = df_feat.drop('HEALTH', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('------------------------')\n",
    "print('TRAIN TEST SPLIT (80/20)')\n",
    "print('------------------------')\n",
    "print('X_train data: ', X_train.shape)\n",
    "print('X_test data: ', X_test.shape)\n",
    "\n",
    "# Train model\n",
    "lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    " \n",
    "# get predicted target from model\n",
    "# Predict on training set\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "print('------------------------')\n",
    "print('TEST SCORES')\n",
    "print('------------------------')\n",
    "\n",
    "# checking accuracy\n",
    "print('logreg Test accuracy score: ', accuracy_score(y_test, lr_pred))\n",
    "\n",
    "# checking f1\n",
    "print('logreg Test f1 score: ', f1_score(y_test, lr_pred))\n",
    "\n",
    "# checking recall\n",
    "print('logreg Test recall score: ', recall_score(y_test, lr_pred))\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "print('------------------------')\n",
    "print('CONFUSION MATRIX')\n",
    "print('------------------------')\n",
    "# generate confusion matrix\n",
    "cm = confusion_matrix(y_test, lr_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print('TOTAL negative (0: \"optimal health\") = ', tn+fn)\n",
    "print('true negative = ', tn)\n",
    "print('false negative = ', fn)\n",
    "\n",
    "print('TOTAL positive (1: \"sub-optimal health\") = ', fp+tp)\n",
    "print('false positive = ', fp)\n",
    "print('true positive = ', tp)\n",
    "\n",
    "# Checking unique values\n",
    "predictions = pd.DataFrame(lr_pred)\n",
    "predictions[0].value_counts()\n",
    "\n",
    "\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try to oversample the minority class (1: \"sub-optimal\" health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "TRAIN TEST SPLIT (80/20)\n",
      "------------------------\n",
      "X_train data:  (5072, 39)\n",
      "X_test data:  (1269, 39)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    2616\n",
       "0    2616\n",
       "Name: HEALTH, dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate input features and target\n",
    "y = df_feat.HEALTH\n",
    "X = df_feat.drop('HEALTH', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('------------------------')\n",
    "print('TRAIN TEST SPLIT (80/20)')\n",
    "print('------------------------')\n",
    "print('X_train data: ', X_train.shape)\n",
    "print('X_test data: ', X_test.shape)\n",
    "\n",
    "# concatenate our training data back together\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "optimal = X[X.HEALTH==0]\n",
    "sub_optimal = X[X.HEALTH==1]\n",
    "\n",
    "# upsample minority\n",
    "sub_upsampled = resample(sub_optimal,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(optimal), # match number in majority class\n",
    "                          random_state=42) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([optimal, sub_upsampled])\n",
    "\n",
    "# check new class counts\n",
    "upsampled.HEALTH.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retry Logistic Regression (using upsampled data). . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "------------------------\n",
      "X_train data:  (5232, 39)\n",
      "X_test data:  (1269, 39)\n",
      "------------------------\n",
      "TEST SCORES\n",
      "------------------------\n",
      "upsampled logreg Test accuracy score:  0.6769109535066982\n",
      "upsampled logreg Test f1 score:  0.6617161716171618\n",
      "upsampled logreg Test recall score:  0.6488673139158576\n",
      "------------------------\n",
      "CONFUSION MATRIX\n",
      "------------------------\n",
      "TOTAL negative (0: \"optimal health\") =  675\n",
      "true negative =  458\n",
      "false negative =  217\n",
      "TOTAL positive (1: \"sub-optimal health\") =  594\n",
      "false positive =  193\n",
      "true positive =  401\n",
      "[[458 193]\n",
      " [217 401]]\n"
     ]
    }
   ],
   "source": [
    "# Separate input features and target\n",
    "y_train = upsampled.HEALTH\n",
    "X_train = upsampled.drop('HEALTH', axis=1)\n",
    "\n",
    "# # setting up testing and training sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('------------------------')\n",
    "# print('TRAIN TEST SPLIT (80/20)')\n",
    "print('------------------------')\n",
    "print('X_train data: ', X_train.shape)\n",
    "print('X_test data: ', X_test.shape)\n",
    "\n",
    "# Train model\n",
    "upsampled = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    " \n",
    "# get predicted target from model\n",
    "# Predict on training set\n",
    "upsampled_pred = upsampled.predict(X_test)\n",
    "\n",
    "print('------------------------')\n",
    "print('TEST SCORES')\n",
    "print('------------------------')\n",
    "\n",
    "# checking accuracy\n",
    "print('upsampled logreg Test accuracy score: ', accuracy_score(y_test, upsampled_pred))\n",
    "\n",
    "# checking f1\n",
    "print('upsampled logreg Test f1 score: ', f1_score(y_test, upsampled_pred))\n",
    "\n",
    "# checking recall\n",
    "print('upsampled logreg Test recall score: ', recall_score(y_test, upsampled_pred))\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "print('------------------------')\n",
    "print('CONFUSION MATRIX')\n",
    "print('------------------------')\n",
    "# generate confusion matrix\n",
    "cm = confusion_matrix(y_test, upsampled_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print('TOTAL negative (0: \"optimal health\") = ', tn+fn)\n",
    "print('true negative = ', tn)\n",
    "print('false negative = ', fn)\n",
    "\n",
    "print('TOTAL positive (1: \"sub-optimal health\") = ', fp+tp)\n",
    "print('false positive = ', fp)\n",
    "print('true positive = ', tp)\n",
    "\n",
    "# Checking unique values\n",
    "predictions = pd.DataFrame(upsampled_pred)\n",
    "predictions[0].value_counts()\n",
    "\n",
    "\n",
    "print(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try to undersample the majority class (0: \"optimal\" health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2456\n",
       "0    2456\n",
       "Name: HEALTH, dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still using our separated classes fraud and not_fraud from above\n",
    "\n",
    "# downsample majority\n",
    "optimal_downsampled = resample(optimal,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(sub_optimal), # match minority n\n",
    "                                random_state = 42) # reproducible results\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([optimal_downsampled, sub_optimal])\n",
    "\n",
    "# checking counts\n",
    "downsampled.HEALTH.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retry Logistic Regression (using downsampled data). . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "------------------------\n",
      "X_train data:  (4912, 39)\n",
      "X_test data:  (1269, 39)\n",
      "------------------------\n",
      "TEST SCORES\n",
      "------------------------\n",
      "undersampled logreg Test accuracy score:  0.6729708431836091\n",
      "undersampled logreg Test f1 score:  0.660670482420278\n",
      "undersampled logreg Test recall score:  0.6537216828478964\n",
      "------------------------\n",
      "CONFUSION MATRIX\n",
      "------------------------\n",
      "TOTAL negative (0: \"optimal health\") =  664\n",
      "true negative =  450\n",
      "false negative =  214\n",
      "TOTAL positive (1: \"sub-optimal health\") =  605\n",
      "false positive =  201\n",
      "true positive =  404\n",
      "[[450 201]\n",
      " [214 404]]\n"
     ]
    }
   ],
   "source": [
    "# Separate input features and target\n",
    "y_train = downsampled.HEALTH\n",
    "X_train = downsampled.drop('HEALTH', axis=1)\n",
    "\n",
    "# # setting up testing and training sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('------------------------')\n",
    "# print('TRAIN TEST SPLIT (80/20)')\n",
    "print('------------------------')\n",
    "print('X_train data: ', X_train.shape)\n",
    "print('X_test data: ', X_test.shape)\n",
    "\n",
    "# Train model\n",
    "undersampled = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    " \n",
    "# get predicted target from model\n",
    "# Predict on training set\n",
    "undersampled_pred = undersampled.predict(X_test)\n",
    "\n",
    "print('------------------------')\n",
    "print('TEST SCORES')\n",
    "print('------------------------')\n",
    "\n",
    "# checking accuracy\n",
    "print('undersampled logreg Test accuracy score: ', accuracy_score(y_test, undersampled_pred))\n",
    "\n",
    "# checking f1\n",
    "print('undersampled logreg Test f1 score: ', f1_score(y_test, undersampled_pred))\n",
    "\n",
    "# checking recall\n",
    "print('undersampled logreg Test recall score: ', recall_score(y_test, undersampled_pred))\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "print('------------------------')\n",
    "print('CONFUSION MATRIX')\n",
    "print('------------------------')\n",
    "# generate confusion matrix\n",
    "cm = confusion_matrix(y_test, undersampled_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print('TOTAL negative (0: \"optimal health\") = ', tn+fn)\n",
    "print('true negative = ', tn)\n",
    "print('false negative = ', fn)\n",
    "\n",
    "print('TOTAL positive (1: \"sub-optimal health\") = ', fp+tp)\n",
    "print('false positive = ', fp)\n",
    "print('true positive = ', tp)\n",
    "\n",
    "# Checking unique values\n",
    "predictions = pd.DataFrame(undersampled_pred)\n",
    "predictions[0].value_counts()\n",
    "\n",
    "\n",
    "print(cm)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train data:  (5072, 39)\n",
      "X_test data:  (1269, 39)\n"
     ]
    }
   ],
   "source": [
    "# Separate input features and target\n",
    "y = df_feat.HEALTH\n",
    "X = df_feat.drop('HEALTH', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('X_train data: ', X_train.shape)\n",
    "print('X_test data: ', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize train and test data\n",
    "# ss = StandardScaler()\n",
    "# X_train_sc = ss.fit_transform(X_train)\n",
    "# X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg train score:  0.699723974763407\n",
      "logreg test score:  0.6832151300236406\n"
     ]
    }
   ],
   "source": [
    "# instantiate logistic regression model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# fit model\n",
    "lr.fit(X_train, y_train)\n",
    "print('logreg train score: ', lr.score(X_train, y_train))\n",
    "print('logreg test score: ', lr.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "TRAIN TEST SPLIT (80/20)\n",
      "------------------------\n",
      "X_train data:  (5072, 39)\n",
      "X_test data:  (1269, 39)\n",
      "------------------------\n",
      "logreg_L1 TEST SCORES\n",
      "------------------------\n",
      "logreg_L1 Test accuracy score:  0.6840031520882585\n",
      "logreg_L1 Test f1 score:  0.6661115736885929\n",
      "logreg_L1 Test recall score:  0.6472491909385113\n",
      "------------------------\n",
      "logreg_L10 TEST SCORES\n",
      "------------------------\n",
      "logreg_L10 Test accuracy score:  0.6840031520882585\n",
      "logreg_L10 Test f1 score:  0.6661115736885929\n",
      "logreg_L10 Test recall score:  0.6472491909385113\n",
      "logreg L1 train score:  0.7003154574132492\n",
      "logreg L1 test score:  0.6840031520882585\n",
      "logreg L10 train score:  0.698935331230284\n",
      "logreg L10 test score:  0.6769109535066982\n",
      "logreg R1 train score:  0.699723974763407\n",
      "logreg R1 test score:  0.6832151300236406\n",
      "logreg R10 train score:  0.701301261829653\n",
      "logreg R10 test score:  0.681639085894405\n"
     ]
    }
   ],
   "source": [
    "# Separate input features and target\n",
    "y = df_feat.HEALTH\n",
    "X = df_feat.drop('HEALTH', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('------------------------')\n",
    "print('TRAIN TEST SPLIT (80/20)')\n",
    "print('------------------------')\n",
    "print('X_train data: ', X_train.shape)\n",
    "print('X_test data: ', X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "# get predicted target from model\n",
    "# Predict on training set\n",
    "L1_pred = logreg_L1.predict(X_test)\n",
    "L10_pred = logreg_L10.predict(X_test)\n",
    "\n",
    "\n",
    "print('------------------------')\n",
    "print('logreg_L1 TEST SCORES')\n",
    "print('------------------------')\n",
    "# checking accuracy\n",
    "print('logreg_L1 Test accuracy score: ', accuracy_score(y_test, L1_pred))\n",
    "# checking f1\n",
    "print('logreg_L1 Test f1 score: ', f1_score(y_test, L1_pred))\n",
    "# checking recall\n",
    "print('logreg_L1 Test recall score: ', recall_score(y_test, L1_pred))\n",
    "print('------------------------')\n",
    "print('logreg_L10 TEST SCORES')\n",
    "print('------------------------')\n",
    "# checking accuracy\n",
    "print('logreg_L10 Test accuracy score: ', accuracy_score(y_test, L1_pred))\n",
    "# checking f1\n",
    "print('logreg_L10 Test f1 score: ', f1_score(y_test, L1_pred))\n",
    "# checking recall\n",
    "print('logreg_L10 Test recall score: ', recall_score(y_test, L1_pred))\n",
    "\n",
    "\n",
    "\n",
    "# instantiate four separate models, \n",
    "# one with LASSO and  ùõº=1, one with LASSO and  ùõº=10,  one with Ridge and  ùõº=1, \n",
    "# and one with Ridge and  ùõº=10\n",
    "# (Hint: Be careful with how you specify  ùõº  in your model!)\n",
    "\n",
    "# \"regularization\"  (adding a penalty):\n",
    "# Lasso = l1, Ridge = l2\n",
    "\n",
    "logreg_L1 = LogisticRegression(penalty = 'l1', C = 1.0)\n",
    "logreg_L10 = LogisticRegression(penalty = 'l1', C = 0.10)\n",
    "logreg_R1 = LogisticRegression(penalty = 'l2', C = 1.0)\n",
    "logreg_R10 = LogisticRegression(penalty = 'l2', C = 0.10)\n",
    "\n",
    "# fit each model\n",
    "logreg_L1.fit(X_train, y_train)\n",
    "print('logreg L1 train score: ', logreg_L1.score(X_train, y_train))\n",
    "print('logreg L1 test score: ', logreg_L1.score(X_test, y_test))\n",
    "\n",
    "logreg_L10.fit(X_train, y_train)\n",
    "print('logreg L10 train score: ', logreg_L10.score(X_train, y_train))\n",
    "print('logreg L10 test score: ', logreg_L10.score(X_test, y_test))\n",
    "\n",
    "logreg_R1.fit(X_train, y_train)\n",
    "print('logreg R1 train score: ', logreg_R1.score(X_train, y_train))\n",
    "print('logreg R1 test score: ', logreg_R1.score(X_test, y_test))\n",
    "\n",
    "logreg_R10.fit(X_train, y_train)\n",
    "print('logreg R10 train score: ', logreg_R10.score(X_train, y_train))\n",
    "print('logreg R10 test score: ', logreg_R10.score(X_test, y_test))\n",
    "\n",
    "# Using accuracy as your metric, evaluate all eight of your models on both the training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg L2 train score:  0.7009069400630915\n",
      "logreg L2 test score:  0.6840031520882585\n",
      "logreg R2 train score:  0.6991324921135647\n",
      "logreg R2 test score:  0.6840031520882585\n"
     ]
    }
   ],
   "source": [
    "# instantiate models, changing value for C (ùõº=2)\n",
    "# \"regularization\"  (adding a penalty):\n",
    "# Lasso = l1, Ridge = l2\n",
    "\n",
    "logreg_L2 = LogisticRegression(penalty = 'l1', C = 2.0)\n",
    "logreg_R2 = LogisticRegression(penalty = 'l2', C = 2.0)\n",
    "\n",
    "# fit each model\n",
    "logreg_L2.fit(X_train, y_train)\n",
    "print('logreg L2 train score: ', logreg_L2.score(X_train, y_train))\n",
    "print('logreg L2 test score: ', logreg_L2.score(X_test, y_test))\n",
    "\n",
    "logreg_R2.fit(X_train, y_train)\n",
    "print('logreg R2 train score: ', logreg_R2.score(X_train, y_train))\n",
    "print('logreg R2 test score: ', logreg_R2.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# The scores are the same as when ùõº=1. \n",
    "# It looks like the balancing effect with regularization is maintaining the same results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train data:  (5072, 39)\n",
      "X_test data:  (1269, 39)\n",
      "Test accuracy score:  0.6493301812450749\n",
      "Test f1 score:  0.6160483175150991\n",
      "Test recall score:  0.5776699029126213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    728\n",
       "1    541\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate input features and target\n",
    "y = df_feat.HEALTH\n",
    "X = df_feat.drop('HEALTH', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('X_train data: ', X_train.shape)\n",
    "print('X_test data: ', X_test.shape)\n",
    "\n",
    "# train model\n",
    "rfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "\n",
    "# checking accuracy\n",
    "print('Test accuracy score: ', accuracy_score(y_test, rfc_pred))\n",
    "\n",
    "# checking f1\n",
    "print('Test f1 score: ', f1_score(y_test, rfc_pred))\n",
    "\n",
    "# checking recall\n",
    "print('Test recall score: ', recall_score(y_test, rfc_pred))\n",
    "\n",
    "# Checking unique values\n",
    "predictions = pd.DataFrame(rfc_pred)\n",
    "predictions[0].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize train and test data\n",
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score:  0.6761229314420804\n",
      "Test f1 score:  0.6313901345291479\n",
      "Test recall score:  0.56957928802589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    772\n",
       "1    497\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# train model\n",
    "rfc = RandomForestClassifier(n_estimators=10).fit(X_train_sc, y_train)\n",
    "\n",
    "# predict on test set\n",
    "rfc_pred = rfc.predict(X_test_sc)\n",
    "\n",
    "# checking accuracy\n",
    "print('Test accuracy score: ', accuracy_score(y_test, rfc_pred))\n",
    "\n",
    "# checking f1\n",
    "print('Test f1 score: ', f1_score(y_test, rfc_pred))\n",
    "\n",
    "# checking recall\n",
    "print('Test recall score: ', recall_score(y_test, rfc_pred))\n",
    "\n",
    "# Checking unique values\n",
    "predictions = pd.DataFrame(rfc_pred)\n",
    "predictions[0].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is a distance-based model (it calculates the distance between neighbors) \n",
    "which is highly sensitive to the magnitude of features.\n",
    "Housing data with features like square footage should be scaled (\"standardized\")\n",
    "\n",
    "https://towardsdatascience.com/when-to-standardize-your-data-in-4-minutes-f9282190707e\n",
    "Standardization makes all variables to contribute equally to the similarity measures\n",
    "\n",
    "k should be odd; it is a hyperparameter we choose and the best number is found through experimentation\n",
    "\n",
    "As k increases, variance decreases (and bias increases).\n",
    "The test score is better than the train score for both KNN models with higher k values (k=15 and k=25)\n",
    "\n",
    "### knn overfitting\n",
    "Models are overfit when the test score is worse than the train score.\n",
    "\n",
    "\n",
    "### In order to address overfitting with KNN, we can try to:\n",
    "### 1. increase the value of k (as seen in the models above)\n",
    "### 2. reduce the number of variables / level of complexity\n",
    "### 3. use a different model (as seen above, Linear Regression performs better...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train data:  (5072, 39)\n",
      "X_test data:  (1269, 39)\n",
      "Test accuracy score:  0.6784869976359338\n"
     ]
    }
   ],
   "source": [
    "# Separate input features and target\n",
    "y = df_feat.HEALTH\n",
    "X = df_feat.drop('HEALTH', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('X_train data: ', X_train.shape)\n",
    "print('X_test data: ', X_test.shape)\n",
    "\n",
    "# Train model  \n",
    "# training a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 71).fit(X_train, y_train)\n",
    "  \n",
    "# accuracy on X_test\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print('Test accuracy score: ', accuracy)\n",
    "\n",
    "# creating a confusion matrix\n",
    "knn_predictions = knn.predict(X_test) \n",
    "cm = confusion_matrix(y_test, knn_predictions)\n",
    "# print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train data:  (5072, 39)\n",
      "X_test data:  (1269, 39)\n",
      "Test accuracy score:  0.6784869976359338\n",
      "[[482 169]\n",
      " [239 379]]\n"
     ]
    }
   ],
   "source": [
    "# Separate input features and target\n",
    "y = df_feat.HEALTH\n",
    "X = df_feat.drop('HEALTH', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('X_train data: ', X_train.shape)\n",
    "print('X_test data: ', X_test.shape)\n",
    "\n",
    "# standardize train and test data\n",
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)\n",
    "\n",
    "\n",
    "# Train model  \n",
    "# training a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 71).fit(X_train_sc, y_train)\n",
    "  \n",
    "# accuracy on X_test\n",
    "accuracy = knn.score(X_test_sc, y_test)\n",
    "print('Test accuracy score: ', accuracy)\n",
    "\n",
    "# creating a confusion matrix\n",
    "knn_predictions = knn.predict(X_test_sc) \n",
    "cm = confusion_matrix(y_test, knn_predictions)\n",
    "# print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate four separate models, \n",
    "# one with  ùëò=3, one with  ùëò=5, one with  ùëò=15, and one with  ùëò=25\n",
    "\n",
    "knn_k3 = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_k5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_k15 = KNeighborsClassifier(n_neighbors=15)\n",
    "knn_k25 = KNeighborsClassifier(n_neighbors=25)\n",
    "\n",
    "# fit each model\n",
    "knn_k3.fit(X_train, y_train)\n",
    "print('KNN 3 train score: ', knn_k3.score(X_train, y_train))\n",
    "print('KNN 3 test score: ', knn_k3.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-e5ccedafdda4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# generate confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'true negative = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# generate confusion matrix\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print('true negative = ', tn)\n",
    "print('false positive = ', fp)\n",
    "print('false negative = ', fn)\n",
    "print('true positive = ', tp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
